{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015f85c9-ca8f-44ff-a2e2-77b08432ed87",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model import Transformer, ModelConfig\n",
    "from trainer import Trainer, TrainerConfig, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer_id = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "checkpoint_path = './model_testing'\n",
    "continue_train = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c8daffc-3f14-45a3-bae6-280eeaf5c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainerConfig(\n",
    "    vocab_size = 50368,\n",
    "    num_epochs = 1,\n",
    "\n",
    "    use_ddp = True,\n",
    "    use_moe = True,\n",
    "    use_lossfreebalance = False,\n",
    "    clean_cuda_cache = True,\n",
    "    use_compile = True,\n",
    "    use_dtype = \"bfloat16\",\n",
    "\n",
    "    seed = 42,\n",
    "    max_seq_len = 128,\n",
    "    batch_size = , \n",
    "    accumulation_steps = 0,\n",
    "    \n",
    "    weight_decay = 0.1,\n",
    "    warmup_ratio = 0.1,\n",
    "    learning_rate = 4e-4,\n",
    "    betas = (0.90, 0.97),\n",
    "    update_rate = 5e-6,\n",
    "\n",
    "    val_ratio = 0.005,\n",
    "    steps_for_eval = 20,\n",
    "    eval_interval = 20,\n",
    "\n",
    "    mlm_probability = 0.30,\n",
    "\n",
    "    checkpoints_frequency = 3500,\n",
    "    path_to_checkpoints = \"./model_testing\",\n",
    "\n",
    "    tokenized_dataset_path = \"\",\n",
    "    hf_dataset_name = \"allenai/c4\",\n",
    "    hf_dataset_config = \"en\",\n",
    "    hf_dataset_split = \"train\",\n",
    "    hf_text_field = \"text\",\n",
    "    hf_add_eos = True,\n",
    "    hf_cache_dir = \"./.cache/hf\",\n",
    "    hf_tokenized_path = \"./.cache/tokenized\",\n",
    "    hf_num_proc = 64,\n",
    "    eval_log_file = \"log/eval.txt\",\n",
    "    use_wandb = True,\n",
    "    wandb_project = \"forschungsprojekt\",\n",
    "    wandb_run_name = \"bert-moe\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e96a43b-1064-4a0a-8b6d-043f1928a24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig(\n",
    "        vocab_size = 50368,\n",
    "\n",
    "        num_dims = 768,\n",
    "        num_heads = 12,\n",
    "        num_kv_heads = 12,\n",
    "        num_layers = 22,\n",
    "        ffn_hidden_dims = 512 * 2,\n",
    "\n",
    "        layernorm_eps = 1e-6,\n",
    "\n",
    "        attention_probs_dropout_prob = 0.0,\n",
    "        attn_qkv_bias = False,\n",
    "        attn_out_bias = False,\n",
    "        attn_out_dropout_prob = 0.0,\n",
    "        global_attn_every_n_layers = 3,\n",
    "        sliding_window = 128,\n",
    "        rotary_emb_base = 10000,\n",
    "        #local_attn_rotary_emb_base = 10000,\n",
    "    \n",
    "        context_len = 128,\n",
    "        \n",
    "        use_cache = False,\n",
    "        use_flash = True,\n",
    "        use_moe = True,\n",
    "\n",
    "        moe_num_experts = 4,\n",
    "        moe_routed_experts = 2,\n",
    "        moe_eps = 1e-6,\n",
    "        moe_aux_loss_coef = 0.01,\n",
    "        moe_shared_experts = 1,\n",
    "        use_lossfreebalance = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66fe4623-632f-4dcb-83a6-7d1500918f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to import flash_attn; defaulting FlexBERT attention implementation to PyTorch's SDPA kernel. This requires padding and unpadding inputs, which will add some overhead.\n",
      "SDPA attention is being used without an attention mask. Including padding in the  attention calculation may cause differences from the Flash Attention implementation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sliding window is not implemented for the PyTorch SDPA path. Use the FA2 backend.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m continue_train:\n\u001b[1;32m      3\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/project/code/model.py:467\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m--> 467\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\u001b[38;5;241m.\u001b[39mappend(\u001b[43mBlock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_id\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mnormalization\u001b[38;5;241m.\u001b[39mRMSNorm(config\u001b[38;5;241m.\u001b[39mnum_dims, config\u001b[38;5;241m.\u001b[39mrmsnorm_eps) \u001b[38;5;66;03m# you also can use RMSNorm(config)\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mll_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/project/code/model.py:417\u001b[0m, in \u001b[0;36mBlock.__init__\u001b[0;34m(self, config, layer_id)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, layer_id: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention \u001b[38;5;241m=\u001b[39m \u001b[43mFlexBertUnpadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39muse_moe:\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn \u001b[38;5;241m=\u001b[39m FFNwMoE(config)\n",
      "File \u001b[0;32m/project/code/model.py:269\u001b[0m, in \u001b[0;36mFlexBertUnpadAttention.__init__\u001b[0;34m(self, config, layer_id)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, layer_id: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn \u001b[38;5;241m=\u001b[39m \u001b[43mFlexBertUnpadRopeAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/project/code/attention.py:181\u001b[0m, in \u001b[0;36mFlexBertUnpadRopeAttention.__init__\u001b[0;34m(self, config, layer_id)\u001b[0m\n\u001b[1;32m    175\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn_once(\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSDPA attention with an attention mask doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use the Flash Attention kernel and will\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m use more memory during the backward pass. Use the FA2 backend for linear memory scaling\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m with sequence length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    179\u001b[0m     )\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msliding_window[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSliding window is not implemented for the PyTorch SDPA path. Use the FA2 backend.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Sliding window is not implemented for the PyTorch SDPA path. Use the FA2 backend."
     ]
    }
   ],
   "source": [
    "model = Transformer(config)\n",
    "if continue_train:\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "    state_dict = checkpoint['model']\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"_orig_mod.\"):\n",
    "            new_state_dict[k[len(\"_orig_mod.\"):]] = v \n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "\n",
    "    model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad71da-f236-4305-af5b-ca9be7a3850f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens loaded: 953,856,000\n",
      "Device: cuda:0\n",
      "Model's trainable params: 236.93M\n",
      "Tokens per step: 262144\n",
      "use torch.compile(): True\n",
      "Use MoE: Yes \n",
      "Number of experts: 4\n",
      "Number of used experts during inference: 2\n",
      "Method of aux_loss: default\n",
      "Number of parameters will be used during inference: 170.85M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.23.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/project/code/wandb/run-20260112_202045-2qlrdf25</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mjerome89-hs-harz/forschungsprojekt/runs/2qlrdf25' target=\"_blank\">smollm-moe</a></strong> to <a href='https://wandb.ai/mjerome89-hs-harz/forschungsprojekt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mjerome89-hs-harz/forschungsprojekt' target=\"_blank\">https://wandb.ai/mjerome89-hs-harz/forschungsprojekt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mjerome89-hs-harz/forschungsprojekt/runs/2qlrdf25' target=\"_blank\">https://wandb.ai/mjerome89-hs-harz/forschungsprojekt/runs/2qlrdf25</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Step: 0 | loss: 11.0879 | acc: 0.0000 | norm: 10.0949 | lr: 2.1046831955922866e-05 | tok/s: 8589.258888476856\n",
      "Epoch: 0 | Step: 1 | loss: 10.5767 | acc: 0.0434 | norm: 6.7429 | lr: 2.2093663911845734e-05 | tok/s: 43202.74503844035\n",
      "Epoch: 0 | Step: 2 | loss: 10.3104 | acc: 0.0514 | norm: 5.3676 | lr: 2.3140495867768598e-05 | tok/s: 43762.14520914456\n",
      "Epoch: 0 | Step: 3 | loss: 10.1071 | acc: 0.0494 | norm: 4.3395 | lr: 2.4187327823691462e-05 | tok/s: 45143.02149718191\n",
      "Epoch: 0 | Step: 4 | loss: 9.9509 | acc: 0.0504 | norm: 3.7357 | lr: 2.523415977961433e-05 | tok/s: 41992.96584693851\n",
      "Epoch: 0 | Step: 5 | loss: 9.8359 | acc: 0.0516 | norm: 3.3117 | lr: 2.628099173553719e-05 | tok/s: 43084.130623821016\n",
      "Epoch: 0 | Step: 6 | loss: 9.7416 | acc: 0.0523 | norm: 2.8601 | lr: 2.7327823691460058e-05 | tok/s: 44413.80966560781\n",
      "Epoch: 0 | Step: 7 | loss: 9.6563 | acc: 0.0509 | norm: 2.5794 | lr: 2.837465564738292e-05 | tok/s: 43724.41909069658\n",
      "Epoch: 0 | Step: 8 | loss: 9.6114 | acc: 0.0527 | norm: 2.4424 | lr: 2.9421487603305787e-05 | tok/s: 45116.18033689736\n",
      "Epoch: 0 | Step: 9 | loss: 9.5596 | acc: 0.0523 | norm: 2.2792 | lr: 3.0468319559228654e-05 | tok/s: 46062.18557658789\n",
      "Epoch: 0 | Step: 10 | loss: 9.5428 | acc: 0.0513 | norm: 2.0653 | lr: 3.151515151515152e-05 | tok/s: 47219.3657279592\n",
      "Epoch: 0 | Step: 11 | loss: 9.5036 | acc: 0.0519 | norm: 2.0007 | lr: 3.256198347107439e-05 | tok/s: 46781.284824996044\n",
      "Epoch: 0 | Step: 12 | loss: 9.4711 | acc: 0.0507 | norm: 1.9461 | lr: 3.360881542699725e-05 | tok/s: 46861.94274989746\n",
      "Epoch: 0 | Step: 13 | loss: 9.4248 | acc: 0.0547 | norm: 1.9709 | lr: 3.465564738292011e-05 | tok/s: 47653.998449982915\n",
      "Epoch: 0 | Step: 14 | loss: 9.4152 | acc: 0.0537 | norm: 1.8581 | lr: 3.5702479338842976e-05 | tok/s: 47190.71602172633\n",
      "Epoch: 0 | Step: 15 | loss: 9.3790 | acc: 0.0524 | norm: 1.8595 | lr: 3.674931129476584e-05 | tok/s: 47640.952418072084\n",
      "Epoch: 0 | Step: 16 | loss: 9.3773 | acc: 0.0512 | norm: 1.7902 | lr: 3.779614325068871e-05 | tok/s: 47943.05362773582\n",
      "Epoch: 0 | Step: 17 | loss: 9.3546 | acc: 0.0514 | norm: 1.7876 | lr: 3.8842975206611575e-05 | tok/s: 47356.068884636436\n",
      "Epoch: 0 | Step: 18 | loss: 9.2904 | acc: 0.0548 | norm: 1.7880 | lr: 3.988980716253444e-05 | tok/s: 47545.69473465416\n",
      "Epoch: 0 | Step: 19 | loss: 9.2407 | acc: 0.0542 | norm: 1.7605 | lr: 4.0936639118457304e-05 | tok/s: 47809.69816827737\n",
      "Epoch: 0 | Step: 20 | loss: 9.2238 | acc: 0.0526 | norm: 1.6974 | lr: 4.198347107438017e-05 | tok/s: 48306.213497183795\n",
      "Epoch: 0 | Step: 21 | loss: 9.1871 | acc: 0.0539 | norm: 1.7060 | lr: 4.303030303030303e-05 | tok/s: 45427.75728872214\n",
      "Epoch: 0 | Step: 22 | loss: 9.1483 | acc: 0.0551 | norm: 1.6932 | lr: 4.40771349862259e-05 | tok/s: 49114.21289672803\n",
      "Epoch: 0 | Step: 23 | loss: 9.1166 | acc: 0.0547 | norm: 1.6435 | lr: 4.512396694214877e-05 | tok/s: 48330.075305314065\n",
      "Epoch: 0 | Step: 24 | loss: 9.0971 | acc: 0.0548 | norm: 1.5772 | lr: 4.6170798898071625e-05 | tok/s: 48972.592534278956\n",
      "Epoch: 0 | Step: 25 | loss: 9.0617 | acc: 0.0530 | norm: 1.5736 | lr: 4.7217630853994496e-05 | tok/s: 48224.563930772405\n",
      "Epoch: 0 | Step: 26 | loss: 9.0067 | acc: 0.0578 | norm: 1.6163 | lr: 4.826446280991736e-05 | tok/s: 48429.61292729363\n",
      "Epoch: 0 | Step: 27 | loss: 8.9891 | acc: 0.0548 | norm: 1.5342 | lr: 4.931129476584022e-05 | tok/s: 48278.41692751954\n",
      "Epoch: 0 | Step: 28 | loss: 8.9317 | acc: 0.0568 | norm: 1.5368 | lr: 5.035812672176309e-05 | tok/s: 48316.587821181034\n",
      "Epoch: 0 | Step: 29 | loss: 8.8817 | acc: 0.0574 | norm: 1.5074 | lr: 5.1404958677685953e-05 | tok/s: 48325.2630075921\n",
      "Epoch: 0 | Step: 30 | loss: 8.8642 | acc: 0.0570 | norm: 1.4644 | lr: 5.245179063360881e-05 | tok/s: 48488.930007733055\n",
      "Epoch: 0 | Step: 31 | loss: 8.8175 | acc: 0.0561 | norm: 1.4490 | lr: 5.349862258953169e-05 | tok/s: 48415.255182153356\n",
      "Epoch: 0 | Step: 32 | loss: 8.7642 | acc: 0.0610 | norm: 1.4916 | lr: 5.4545454545454546e-05 | tok/s: 48045.1665649235\n",
      "Epoch: 0 | Step: 33 | loss: 8.7235 | acc: 0.0596 | norm: 1.4791 | lr: 5.5592286501377424e-05 | tok/s: 47345.57871035003\n",
      "Epoch: 0 | Step: 34 | loss: 8.7119 | acc: 0.0495 | norm: 2.1843 | lr: 5.6639118457300275e-05 | tok/s: 47548.073786783316\n",
      "Epoch: 0 | Step: 35 | loss: 8.6303 | acc: 0.0611 | norm: 2.0906 | lr: 5.7685950413223146e-05 | tok/s: 47955.34784309466\n",
      "Epoch: 0 | Step: 36 | loss: 8.5958 | acc: 0.0598 | norm: 1.5209 | lr: 5.873278236914601e-05 | tok/s: 47389.41088953776\n",
      "Epoch: 0 | Step: 37 | loss: 8.5944 | acc: 0.0528 | norm: 2.0736 | lr: 5.977961432506887e-05 | tok/s: 47011.839303630506\n",
      "Epoch: 0 | Step: 38 | loss: 8.5286 | acc: 0.0573 | norm: 3.6076 | lr: 6.0826446280991745e-05 | tok/s: 46366.344206287264\n",
      "Epoch: 0 | Step: 39 | loss: 8.5043 | acc: 0.0488 | norm: 1.8204 | lr: 6.18732782369146e-05 | tok/s: 47236.41331677686\n",
      "Epoch: 0 | Step: 40 | loss: 8.4515 | acc: 0.0602 | norm: 1.4004 | lr: 6.292011019283747e-05 | tok/s: 47067.98642403946\n",
      "Epoch: 0 | Step: 41 | loss: 8.4093 | acc: 0.0595 | norm: 2.3194 | lr: 6.396694214876034e-05 | tok/s: 41967.168442663286\n",
      "Epoch: 0 | Step: 42 | loss: 8.3822 | acc: 0.0520 | norm: 2.0592 | lr: 6.50137741046832e-05 | tok/s: 45964.471976845605\n",
      "Epoch: 0 | Step: 43 | loss: 8.3103 | acc: 0.0620 | norm: 1.5082 | lr: 6.606060606060607e-05 | tok/s: 46446.421768642824\n",
      "Epoch: 0 | Step: 44 | loss: 8.2956 | acc: 0.0618 | norm: 1.4674 | lr: 6.710743801652892e-05 | tok/s: 45530.49695331321\n",
      "Epoch: 0 | Step: 45 | loss: 8.2565 | acc: 0.0648 | norm: 2.0352 | lr: 6.81542699724518e-05 | tok/s: 45412.37940267002\n",
      "Epoch: 0 | Step: 46 | loss: 8.2388 | acc: 0.0624 | norm: 2.6396 | lr: 6.920110192837465e-05 | tok/s: 41751.4300618207\n",
      "Epoch: 0 | Step: 47 | loss: 8.1481 | acc: 0.0792 | norm: 1.2219 | lr: 7.024793388429754e-05 | tok/s: 44148.09997902167\n",
      "Epoch: 0 | Step: 48 | loss: 8.1619 | acc: 0.0753 | norm: 2.2737 | lr: 7.12947658402204e-05 | tok/s: 44634.501573233196\n",
      "Epoch: 0 | Step: 49 | loss: 8.0942 | acc: 0.0856 | norm: 1.7508 | lr: 7.234159779614325e-05 | tok/s: 42084.35393432634\n",
      "Epoch: 0 | Step: 50 | loss: 8.0724 | acc: 0.0641 | norm: 2.3012 | lr: 7.338842975206612e-05 | tok/s: 41752.38490904272\n",
      "Epoch: 0 | Step: 51 | loss: 8.0329 | acc: 0.0816 | norm: 1.6049 | lr: 7.443526170798898e-05 | tok/s: 39627.10321446947\n",
      "Epoch: 0 | Step: 52 | loss: 8.0376 | acc: 0.0823 | norm: 3.4778 | lr: 7.548209366391185e-05 | tok/s: 41115.476645951276\n",
      "Epoch: 0 | Step: 53 | loss: 7.9858 | acc: 0.0749 | norm: 2.2392 | lr: 7.652892561983471e-05 | tok/s: 40212.45527563318\n",
      "Epoch: 0 | Step: 54 | loss: 7.9611 | acc: 0.0771 | norm: 1.6138 | lr: 7.75757575757576e-05 | tok/s: 42644.691116047135\n",
      "Epoch: 0 | Step: 55 | loss: 7.9069 | acc: 0.0848 | norm: 1.8442 | lr: 7.862258953168045e-05 | tok/s: 38957.68880162236\n",
      "Epoch: 0 | Step: 56 | loss: 7.8877 | acc: 0.0868 | norm: 1.3036 | lr: 7.96694214876033e-05 | tok/s: 41631.94306068704\n",
      "Epoch: 0 | Step: 57 | loss: 7.8360 | acc: 0.0821 | norm: 1.5822 | lr: 8.071625344352618e-05 | tok/s: 40194.09538903571\n",
      "Epoch: 0 | Step: 58 | loss: 7.8005 | acc: 0.0846 | norm: 1.6823 | lr: 8.176308539944904e-05 | tok/s: 40675.24749593669\n",
      "Epoch: 0 | Step: 59 | loss: 7.7858 | acc: 0.0872 | norm: 1.5499 | lr: 8.280991735537191e-05 | tok/s: 40200.05947021617\n",
      "Epoch: 0 | Step: 60 | loss: 7.7738 | acc: 0.0865 | norm: 0.9469 | lr: 8.385674931129477e-05 | tok/s: 41008.97180560774\n",
      "Epoch: 0 | Step: 61 | loss: 7.7268 | acc: 0.0845 | norm: 1.2741 | lr: 8.490358126721762e-05 | tok/s: 38295.15665139596\n",
      "Epoch: 0 | Step: 62 | loss: 7.7135 | acc: 0.0874 | norm: 1.2302 | lr: 8.595041322314051e-05 | tok/s: 39895.990229358395\n",
      "Epoch: 0 | Step: 63 | loss: 7.6836 | acc: 0.0904 | norm: 1.1772 | lr: 8.699724517906335e-05 | tok/s: 40533.4360935774\n",
      "Epoch: 0 | Step: 64 | loss: 7.6678 | acc: 0.0875 | norm: 0.8886 | lr: 8.804407713498624e-05 | tok/s: 39806.77937739655\n",
      "Epoch: 0 | Step: 65 | loss: 7.6428 | acc: 0.0905 | norm: 1.1047 | lr: 8.90909090909091e-05 | tok/s: 39558.131900156186\n",
      "Epoch: 0 | Step: 66 | loss: 7.6238 | acc: 0.0925 | norm: 1.4887 | lr: 9.013774104683197e-05 | tok/s: 39472.95141985754\n",
      "Epoch: 0 | Step: 67 | loss: 7.6063 | acc: 0.0948 | norm: 1.3384 | lr: 9.118457300275482e-05 | tok/s: 38039.7305468223\n",
      "Epoch: 0 | Step: 68 | loss: 7.5369 | acc: 0.0967 | norm: 0.9804 | lr: 9.223140495867768e-05 | tok/s: 37736.89881335652\n",
      "Epoch: 0 | Step: 69 | loss: 7.5152 | acc: 0.1008 | norm: 1.4493 | lr: 9.327823691460056e-05 | tok/s: 36550.641867525665\n",
      "Epoch: 0 | Step: 70 | loss: 7.5331 | acc: 0.1042 | norm: 1.1798 | lr: 9.432506887052341e-05 | tok/s: 36656.91773517871\n",
      "Epoch: 0 | Step: 71 | loss: 7.5051 | acc: 0.1041 | norm: 1.8030 | lr: 9.53719008264463e-05 | tok/s: 34707.31984815469\n",
      "Epoch: 0 | Step: 72 | loss: 7.4733 | acc: 0.1022 | norm: 1.5366 | lr: 9.641873278236915e-05 | tok/s: 36499.17497675275\n",
      "Epoch: 0 | Step: 73 | loss: 7.5080 | acc: 0.1055 | norm: 1.4182 | lr: 9.746556473829202e-05 | tok/s: 35018.29037550212\n",
      "Epoch: 0 | Step: 74 | loss: 7.4387 | acc: 0.1020 | norm: 1.5585 | lr: 9.851239669421488e-05 | tok/s: 34456.14790888251\n",
      "Epoch: 0 | Step: 75 | loss: 7.3763 | acc: 0.1128 | norm: 0.9729 | lr: 9.955922865013774e-05 | tok/s: 36123.41443498132\n",
      "Epoch: 0 | Step: 76 | loss: 7.4030 | acc: 0.1153 | norm: 1.2357 | lr: 0.00010060606060606062 | tok/s: 36897.49086948228\n",
      "Epoch: 0 | Step: 77 | loss: 7.3748 | acc: 0.1157 | norm: 1.4147 | lr: 0.00010165289256198347 | tok/s: 34418.04833844436\n",
      "Epoch: 0 | Step: 78 | loss: 7.3829 | acc: 0.1176 | norm: 2.3323 | lr: 0.00010269972451790635 | tok/s: 35083.20723139665\n",
      "Epoch: 0 | Step: 79 | loss: 7.3250 | acc: 0.1235 | norm: 1.6009 | lr: 0.00010374655647382921 | tok/s: 36036.33775167562\n",
      "Epoch: 0 | Step: 80 | loss: 7.3104 | acc: 0.1172 | norm: 2.1582 | lr: 0.00010479338842975206 | tok/s: 35711.76668342031\n",
      "Epoch: 0 | Step: 81 | loss: 7.3440 | acc: 0.1233 | norm: 1.7361 | lr: 0.00010584022038567494 | tok/s: 33150.46708697219\n",
      "Epoch: 0 | Step: 82 | loss: 7.3249 | acc: 0.1210 | norm: 2.3327 | lr: 0.0001068870523415978 | tok/s: 37044.88655295204\n",
      "Epoch: 0 | Step: 83 | loss: 7.3319 | acc: 0.1264 | norm: 2.0108 | lr: 0.00010793388429752066 | tok/s: 35588.205045069284\n",
      "Epoch: 0 | Step: 84 | loss: 7.2297 | acc: 0.1354 | norm: 1.4719 | lr: 0.00010898071625344352 | tok/s: 36032.431394097075\n",
      "Epoch: 0 | Step: 85 | loss: 7.2335 | acc: 0.1342 | norm: 1.5749 | lr: 0.0001100275482093664 | tok/s: 35631.45719485064\n",
      "Epoch: 0 | Step: 86 | loss: 7.2577 | acc: 0.1340 | norm: 2.0695 | lr: 0.00011107438016528926 | tok/s: 35309.99885712821\n",
      "Epoch: 0 | Step: 87 | loss: 7.2100 | acc: 0.1380 | norm: 1.1561 | lr: 0.00011212121212121212 | tok/s: 35867.70629554523\n",
      "Epoch: 0 | Step: 88 | loss: 7.2462 | acc: 0.1350 | norm: 2.4689 | lr: 0.00011316804407713499 | tok/s: 34513.19983391475\n",
      "Epoch: 0 | Step: 89 | loss: 7.2120 | acc: 0.1361 | norm: 1.5876 | lr: 0.00011421487603305785 | tok/s: 35456.005141102905\n",
      "Epoch: 0 | Step: 90 | loss: 7.2123 | acc: 0.1384 | norm: 2.4066 | lr: 0.00011526170798898071 | tok/s: 35466.91363726796\n",
      "Epoch: 0 | Step: 91 | loss: 7.1877 | acc: 0.1399 | norm: 1.6811 | lr: 0.00011630853994490358 | tok/s: 36358.399171708515\n",
      "Epoch: 0 | Step: 92 | loss: 7.1841 | acc: 0.1364 | norm: 2.8534 | lr: 0.00011735537190082644 | tok/s: 35190.43894804213\n",
      "Epoch: 0 | Step: 93 | loss: 7.1612 | acc: 0.1397 | norm: 1.7711 | lr: 0.00011840220385674932 | tok/s: 32739.607643404586\n",
      "Epoch: 0 | Step: 94 | loss: 7.1790 | acc: 0.1433 | norm: 1.9543 | lr: 0.00011944903581267218 | tok/s: 35165.077423815994\n",
      "Epoch: 0 | Step: 95 | loss: 7.1156 | acc: 0.1480 | norm: 2.3850 | lr: 0.00012049586776859505 | tok/s: 34239.23129452738\n",
      "Epoch: 0 | Step: 96 | loss: 7.0968 | acc: 0.1480 | norm: 2.2667 | lr: 0.00012154269972451791 | tok/s: 35948.906872837935\n",
      "Epoch: 0 | Step: 97 | loss: 7.0674 | acc: 0.1471 | norm: 3.1964 | lr: 0.00012258953168044076 | tok/s: 33324.85518087241\n",
      "Epoch: 0 | Step: 98 | loss: 7.0821 | acc: 0.1487 | norm: 2.0722 | lr: 0.0001236363636363636 | tok/s: 34477.10573528386\n",
      "Epoch: 0 | Step: 99 | loss: 7.0702 | acc: 0.1501 | norm: 2.3884 | lr: 0.0001246831955922865 | tok/s: 35532.601547000966\n",
      "Epoch: 0 | Step: 100 | loss: 6.9979 | acc: 0.1556 | norm: 2.2136 | lr: 0.00012573002754820935 | tok/s: 32991.91289196548\n",
      "Epoch: 0 | Step: 101 | loss: 7.0139 | acc: 0.1541 | norm: 2.0185 | lr: 0.00012677685950413222 | tok/s: 33625.800276549984\n",
      "Epoch: 0 | Step: 102 | loss: 6.9608 | acc: 0.1608 | norm: 2.0838 | lr: 0.0001278236914600551 | tok/s: 32998.650726730426\n",
      "Epoch: 0 | Step: 103 | loss: 6.9776 | acc: 0.1616 | norm: 1.4151 | lr: 0.00012887052341597796 | tok/s: 34711.57220814585\n",
      "Epoch: 0 | Step: 104 | loss: 6.9919 | acc: 0.1577 | norm: 2.0526 | lr: 0.0001299173553719008 | tok/s: 35031.191839017796\n",
      "Epoch: 0 | Step: 105 | loss: 6.9361 | acc: 0.1632 | norm: 2.1912 | lr: 0.0001309641873278237 | tok/s: 35762.54406065033\n",
      "Epoch: 0 | Step: 106 | loss: 6.9221 | acc: 0.1672 | norm: 2.6919 | lr: 0.00013201101928374655 | tok/s: 35542.37208719473\n",
      "Epoch: 0 | Step: 107 | loss: 6.8685 | acc: 0.1673 | norm: 1.8417 | lr: 0.00013305785123966942 | tok/s: 34483.929427772375\n",
      "Epoch: 0 | Step: 108 | loss: 6.8640 | acc: 0.1694 | norm: 1.3804 | lr: 0.0001341046831955923 | tok/s: 35149.31284802038\n",
      "Epoch: 0 | Step: 109 | loss: 6.8500 | acc: 0.1719 | norm: 2.4678 | lr: 0.00013515151515151516 | tok/s: 36036.87557754638\n",
      "Epoch: 0 | Step: 110 | loss: 6.8309 | acc: 0.1730 | norm: 2.1428 | lr: 0.000136198347107438 | tok/s: 34998.401089856845\n",
      "Epoch: 0 | Step: 111 | loss: 6.8395 | acc: 0.1713 | norm: 2.5905 | lr: 0.00013724517906336088 | tok/s: 34529.76368281313\n",
      "Epoch: 0 | Step: 112 | loss: 6.8551 | acc: 0.1758 | norm: 2.4950 | lr: 0.00013829201101928372 | tok/s: 34775.95662838366\n",
      "Epoch: 0 | Step: 113 | loss: 6.8108 | acc: 0.1774 | norm: 1.9022 | lr: 0.00013933884297520662 | tok/s: 34921.018552832895\n",
      "Epoch: 0 | Step: 114 | loss: 6.7616 | acc: 0.1774 | norm: 2.6688 | lr: 0.00014038567493112946 | tok/s: 35015.84366878518\n",
      "Epoch: 0 | Step: 115 | loss: 6.7646 | acc: 0.1756 | norm: 2.5062 | lr: 0.00014143250688705233 | tok/s: 34825.33471303772\n",
      "Epoch: 0 | Step: 116 | loss: 6.7216 | acc: 0.1795 | norm: 2.1506 | lr: 0.0001424793388429752 | tok/s: 35819.56732317689\n",
      "Epoch: 0 | Step: 117 | loss: 6.7168 | acc: 0.1824 | norm: 2.3715 | lr: 0.00014352617079889808 | tok/s: 35694.31220767735\n",
      "Epoch: 0 | Step: 118 | loss: 6.6557 | acc: 0.1883 | norm: 2.9797 | lr: 0.00014457300275482092 | tok/s: 33665.67647782897\n",
      "Epoch: 0 | Step: 119 | loss: 6.6976 | acc: 0.1858 | norm: 1.6433 | lr: 0.00014561983471074382 | tok/s: 35732.01839592105\n",
      "Epoch: 0 | Step: 120 | loss: 6.6443 | acc: 0.1904 | norm: 2.0516 | lr: 0.00014666666666666666 | tok/s: 35569.03425987981\n",
      "Epoch: 0 | Step: 121 | loss: 6.6712 | acc: 0.1875 | norm: 3.2418 | lr: 0.00014771349862258953 | tok/s: 33195.33892594584\n",
      "Epoch: 0 | Step: 122 | loss: 6.6051 | acc: 0.1940 | norm: 1.8068 | lr: 0.0001487603305785124 | tok/s: 36525.18816220398\n",
      "Epoch: 0 | Step: 123 | loss: 6.5770 | acc: 0.1970 | norm: 1.9719 | lr: 0.00014980716253443525 | tok/s: 35458.6520919262\n",
      "Epoch: 0 | Step: 124 | loss: 6.5430 | acc: 0.1980 | norm: 2.1792 | lr: 0.00015085399449035812 | tok/s: 35300.10691693405\n",
      "Epoch: 0 | Step: 125 | loss: 6.5119 | acc: 0.2017 | norm: 2.3733 | lr: 0.000151900826446281 | tok/s: 34635.918985803386\n",
      "Epoch: 0 | Step: 126 | loss: 6.5299 | acc: 0.2018 | norm: 1.8320 | lr: 0.00015294765840220383 | tok/s: 34195.74677808581\n",
      "Epoch: 0 | Step: 127 | loss: 6.5139 | acc: 0.1968 | norm: 3.3732 | lr: 0.00015399449035812673 | tok/s: 35195.080923342925\n",
      "Epoch: 0 | Step: 128 | loss: 6.4994 | acc: 0.2029 | norm: 1.7310 | lr: 0.00015504132231404958 | tok/s: 35504.40170561397\n",
      "Epoch: 0 | Step: 129 | loss: 6.4596 | acc: 0.2079 | norm: 4.6526 | lr: 0.00015608815426997245 | tok/s: 35418.06915207473\n",
      "Epoch: 0 | Step: 130 | loss: 6.4501 | acc: 0.2084 | norm: 1.9447 | lr: 0.00015713498622589532 | tok/s: 34457.884443233954\n",
      "Epoch: 0 | Step: 131 | loss: 6.4246 | acc: 0.2104 | norm: 1.6254 | lr: 0.0001581818181818182 | tok/s: 34589.33913397746\n",
      "Epoch: 0 | Step: 132 | loss: 6.3804 | acc: 0.2156 | norm: 3.6270 | lr: 0.00015922865013774103 | tok/s: 36194.17950150249\n",
      "Epoch: 0 | Step: 133 | loss: 6.3913 | acc: 0.2145 | norm: 2.3083 | lr: 0.00016027548209366393 | tok/s: 36204.89136746722\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(train_config, tokenizer=tokenizer)\n",
    "trainer = Trainer(train_config, model, tokenizer)\n",
    "trainer.train(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b697b-7940-4b9f-898f-8e0a5bc5a163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
